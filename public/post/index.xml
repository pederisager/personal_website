<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on Peder M. Isager</title>
    <link>/post/</link>
    <description>Recent content in Posts on Peder M. Isager</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2018</copyright>
    <lastBuildDate>Sun, 01 Jan 2017 00:00:00 +0100</lastBuildDate>
    
	<atom:link href="/post/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Fifty ways to leave your model</title>
      <link>/post/fifty-ways-to-leave-your-model/</link>
      <pubDate>Sat, 15 Feb 2020 00:00:00 +0000</pubDate>
      
      <guid>/post/fifty-ways-to-leave-your-model/</guid>
      <description>The problem is all inside your head, she said to me. The answer is easy if you take it logically. I’d like to help you in your struggle to be free. There must be fifty2.2162 ways to leave your lovermodel.
Suppose we have a theory about the mechanism that sustains insomnia. The theory can be roughly summarized in a (dynamic) causal graph model like so:
Figure 1: Example causal model.</description>
    </item>
    
    <item>
      <title>Meta-research at the Psychological Science Accelerator</title>
      <link>/post/meta-research-at-the-psa/</link>
      <pubDate>Tue, 17 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/meta-research-at-the-psa/</guid>
      <description>(This is a reposting of a blog post originally posted to: https://metaresearch.nl/blog/2019/12/16/meta-research-at-the-psychological-science-accelerator)
Friday November 22, 2019, the Meta-research center at Tilburg University (https://metaresearch.nl/) organized the meta-research day. Around 90 researchers attended the meta-research day that involved three plenary lectures, by John Ioannidis (who received an honorary doctorate from Tilburg University a day earlier), Ana Marušić, and Sarah de Rijcke, and seven parallel sessions on meta-research (https://www.tilburguniversity.edu/about/schools/socialsciences/organization/departments/methodology-statistics/colloquia/meta-research-day). One of these sessions was titled How can meta-research improve the Psychological Science Accelerator (PSA) and how can the PSA improve meta-research?</description>
    </item>
    
    <item>
      <title>Mixed model equivalence test using R and PANGEA</title>
      <link>/post/mixed_model_equivalence/</link>
      <pubDate>Mon, 25 Nov 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/mixed_model_equivalence/</guid>
      <description>How to calculate an equivalence test, and power for an equivalence test, for a fixed effect in a mixed effects model using R and PANGEA.
Introduction to a two-part problemCalculating equivalence test (TOST) power for fixed effects in a mixed model can be compartmentalized into two independent problems. First, we need a method for calculating TOST power from the non-centrality parameters (ncp) and degrees of freedom (df) of a test.</description>
    </item>
    
    <item>
      <title>Proposing a theory committee at the Psychological Science Accelerator</title>
      <link>/post/psa-theory-committee/</link>
      <pubDate>Sun, 25 Aug 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/psa-theory-committee/</guid>
      <description>(written in collaboration with Nicholas A. Coles)
The Psychological Science Accelerator (PSA) is a global network of psychological laboratories that organizes large-scale empirical research projects. The organization aims “to accelerate the accumulation of reliable and generalizable evidence in psychological science” (Moshontz et al. 2018). To succeed in this mission, the PSA has created several committees that work together to address important challenges in producing high quality research, such as having access to an inferentially meaningful sample size, reducing the reliance on WEIRD participants, and improving the methodological quality of designs and analyses.</description>
    </item>
    
    <item>
      <title>Quantifying the corroboration of a finding</title>
      <link>/post/quantifying-the-corroboration-of-a-finding/</link>
      <pubDate>Fri, 12 Apr 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/quantifying-the-corroboration-of-a-finding/</guid>
      <description>AbstractIn a fourthcoming paper by members of the Open Science Collaboration, we outline how one could use quantitative estimates of “replication value” to guide study selection in replication research. We define replication value conceptually as a ratio of “impact” over “corroboration”, and we show how to derive quantifiable estimates from this definition. Here I will try to identify a reasonable and quantitative operationalization of corroboration. I first propose to approximate corroboration through the more narrow concept “precision of the estimate” (figure 1).</description>
    </item>
    
    <item>
      <title>How to share your data online with OSF</title>
      <link>/post/how-to-share-your-data-with-osf/</link>
      <pubDate>Tue, 02 Oct 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/how-to-share-your-data-with-osf/</guid>
      <description>Disclaimer:In the time I spent writing up this blog post, Courtney Soderberg published a tutorial manuscript for data sharing on OSF in the journal AMPPS. I highly recommend checking it out!
IntroThe Open Science movement is here to stay. Journals, reviewers, and fellow scientists increasingly expect that data and materials from research findings be made available upon publication. There are very good reasons for this, as open data and materials have the potential to increase reproducibility and collaboration, reduce the cost of data collection, and shift the focus from single findings to cumulative research programs.</description>
    </item>
    
    <item>
      <title>Ganging Up on Psychological Science</title>
      <link>/post/ganging-up-on-psych-science/</link>
      <pubDate>Fri, 31 Aug 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/ganging-up-on-psych-science/</guid>
      <description>Everything is awesome. Everything is cool when you’re part of a team.The idea of having heaps of scientists come together to accomplish something none of them could do on their own is not new. Take CERN, established in 1954 and one of the most famous large scale collaborations in science to date. CERN is a massive experimental undertaking with a mission to address fundamental problems in physics. Today, it employs over 2500 scientists from 22 countries and has lead to some pretty impressive innovations, like the construction of the Large Hadron Collider (the world’s biggest machine), the discovery of the Higgs boson, and the invention of the Word Wide Web.</description>
    </item>
    
    <item>
      <title>What to Replicate?</title>
      <link>/post/what-to-replicate/</link>
      <pubDate>Sat, 09 Jun 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/what-to-replicate/</guid>
      <description>Justifications of study choice from 85 replication studies.In their recent BBS article, Zwaan et al. (2017) synthesized arguments for and against replication research and laid out a convincing argument for the value of direct replication in science. My collaborators and I wrote a commentary on this article (Coles et al. 2018). In it, we argued that in order to maximize the utility of replication in a science that is operating under resource constraints (there is only so much time and money for doing research), we need to deal with the question of when to replicate.</description>
    </item>
    
  </channel>
</rss>