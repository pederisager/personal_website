% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[
]{article}
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\hypersetup{
  pdftitle={What is Worth Replicating? - SIPS 2020 Unconference session},
  pdfauthor={Peder M. Isager \& Anna van `t Veer},
  hidelinks,
  pdfcreator={LaTeX via pandoc}}
\urlstyle{same} % disable monospaced font for URLs
\usepackage[margin=1in]{geometry}
\usepackage{graphicx,grffile}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{-\maxdimen} % remove section numbering

\title{What is Worth Replicating? - SIPS 2020 Unconference session}
\author{Peder M. Isager \& Anna van `t Veer}
\date{2020-06-30}

\begin{document}
\maketitle

Given that most research is original and we have limited resources
available for replication, we need guidelines for study selection in
replication research. But what makes a study worth replicating? In our
unconference session at SIPS this year, we had the pleasure of
discussing this problem with fifty-some enthusiastic scholars. Here is a
quick summary of what they taught us.

First off, we went into this session with our own prior beliefs about
what makes a study worth replicating. Namely, we strongly believe that
studies are worth replicating (1) if their results are uncertain, (2) if
we can reduce uncertainty about the results by replicating, and (3) the
results are worth becoming less uncertain about.

In part, this session was an opportunity for us to see if our beliefs
are shared by others. In part, it gave us an opportunity to fish for
blind spots in our own beliefs; perhaps there are important factors for
determining what is worth replicating that we have not thought of?

To get the discussion going, we asked participants to focus on one of
three questions:

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\tightlist
\item
  Which specific studies/claims in your field of interest do you believe
  currently are the most in need of replication? Why these?
\item
  What criteria (general or specific to your field of interest) would
  you use to determine which studies in your field to spend resources
  replicating?
\item
  Have you ever conducted a replication? If so, how did you go about
  selecting a study to replicate? Did you need to select one of multiple
  candidates? If so, how did you choose among them?
\end{enumerate}

\hypertarget{factors-mentioned-that-jive-with-our-prior-beliefs}{%
\subsection{Factors mentioned that jive with our prior
beliefs:}\label{factors-mentioned-that-jive-with-our-prior-beliefs}}

The discussion was mainly focused on general factors that contribute to
determine replication worthiness (question 2). From the notes and
summaries provided by the groups at the end of the session, we find that
many of the factors that people brought up are related to the framework
we had in mind going into the session. I.e. the uncertainty about
existing knowledge, the value of having knowledge, and the possibility
of gaining knowledge through replication, all seem to be factors that
people brought up as important. It is also abundantly clear from the
discussion that these factors are complicated, and cannot easily be tied
down to any specific set of operationalizations. Here are just some of
the factors that people brought up as potentially important for
considering if something is worth replicating (\emph{italicized indented
bullet points indicate our post-session replies to comments}):

\hypertarget{uncertain-about-existing-knowledge}{%
\subsubsection{Uncertain about existing
knowledge}\label{uncertain-about-existing-knowledge}}

\begin{itemize}
\item
  Distrust in how the original study was run
\item
  Statistical uncertainty (e.g.~as measured by confidence intervals)
\item
  Distrust in the researcher/lab that conducted the study
\item
  Generalizability of results
\item
  Replicability
\item
  Small sample size/low statistical power
\item
  Suspicious data/prevalence of p-hacking
\item
  Results only replicable in a single lab
\item
  Methodological details missing from the report
\item
  ``Fragile'' findings
\item
  HARKed conclusions
\item
  Multiple explanations for the finding exist

  \begin{itemize}
  \tightlist
  \item
    \emph{Reply: Here I think it is worth considering if a replication
    can actually help us mitigate the problem. If a study design leaves
    open multiple explanations for the results, and we simply replicate
    the design with more data, we may still not be able to separate
    between different explanations when we see our results. Thus, this
    kind of uncertainty might actually call for novel study designs
    rather than replication (e.g., in an adversarial collaboration).}
  \end{itemize}
\item
  Degree of available evidence supporting the phenomenon
\item
  Certainty in result
\end{itemize}

\hypertarget{our-ability-to-reduce-uncertainty}{%
\subsubsection{Our ability to reduce
uncertainty}\label{our-ability-to-reduce-uncertainty}}

\begin{itemize}
\item
  Feasibility
\item
  Expense/Resource cost
\item
  Availability of materials
\item
  Replication estimates the efficacy and generalisability of specific
  stimuli
\item
  Replication estimates the degree to which an effect exists `in the
  wild'
\item
  Availability of stimuli, resources, full explanation of methods, etc.

  \begin{itemize}
  \tightlist
  \item
    \emph{Reply: Although most replicators will of course strive for a
    rigorous replication, some replication studies might not add much in
    terms of reducing uncertainty (e.g.~if no resources are available to
    achieve adequate power). It seems most of these bullets assume the
    replication itself is `good' but we should stay critical there as
    well. For selecting a study to replicate, the question of how much
    uncertainty the replication can reduce may for instance be raised
    when phase 1 reviewers judge a replication proposal.}
  \end{itemize}
\item
  ``Value of becoming certain''
\item
  Scientific impact
\item
  ``Real world'' consequences
\item
  Number of citations

  \begin{itemize}
  \tightlist
  \item
    \emph{Reply: However, as many pointed out, citation count is not a
    perfect indicator of impact, and some citations have little to do
    with the impact of the paper on a field. }
  \end{itemize}
\item
  Impactfulness
\item
  Interesting study
\item
  Study design one intends to extend
\item
  Societal impact. (e.g.~on public consumption, policy,
  interest/awareness/attention in media)
\item
  Number of scientists working on the topic? To avoid lots of wasted
  resources.
\item
  Number of people who could benefit from the knowledge.
\item
  International impact.
\item
  Potential benefit/harm for vulnerable populations.
\item
  Relevance (personal/theoretical/political)
\end{itemize}

There was also a group that discussed study selection strategies in past
replication research (question 3). The criteria mentioned by this group
as having been important for actual study selection decisions largely
overlapped with the factors listed above.

\hypertarget{factors-mentioned-that-do-not-obviously-jive-with-our-prior-beliefs.}{%
\subsection{Factors mentioned that do not obviously jive with our prior
beliefs.}\label{factors-mentioned-that-do-not-obviously-jive-with-our-prior-beliefs.}}

It is always useful to understand when a normative (what should be
replicated) framework does not coincide with descriptive reality (what
is replicated), or when different normative frameworks collide. Here are
a few examples of factors people brought up, that do not obviously fit
within our three-category system for deciding what is worth replicating:

\begin{itemize}
\item
  Focus on replicating old studies vs new studies

  \begin{itemize}
  \tightlist
  \item
    \emph{Reply: It is perhaps reasonable to assume that age of the
    study is correlated with the importance of the study for a field of
    research. Older studies have been in circulation for longer, and
    have had more time to become embedded in the research canon.
    However, some study designs and results become outdated with age
    (e.g.~through improved study designs, or by having conclusions
    falsified by subsequent research). Thus, you could also argue that
    more recent studies are more likely to be influencing current
    theorizing and should be prioritized for replication. Both are
    perhaps equally sound conclusions in different scenarios, which
    would entail that there is no straight-forward relationship between
    age and replication worthiness of a study. }
  \end{itemize}
\item
  Finding is surprising/violates common sense

  \begin{itemize}
  \tightlist
  \item
    \emph{Reply: It is not completely clear to me whether this factor is
    mentioned because it entails uncertainty, because findings that
    clash with our intuitions are more valuable, or for some other
    reason. I agree that it is likely important, and certainly has been
    the motivation for several actual replication efforts. It would be
    interesting to dig further into why/when a surprising finding ought
    to be more worth replicating than an unsurprising one. }
  \end{itemize}
\item
  Surveying the `health' of a discipline as a whole

  \begin{itemize}
  \tightlist
  \item
    \emph{Reply: A good example of a replication project with this goal
    in mind is the Reproducibility Project Psychology. In this project,
    even though authors were still motivated by things like impact and
    feasibility (see their reported study inclusion criteria), it is
    clear that their motivation was partly to get a representative
    sample of findings from the field. In this case, the desire to
    identify studies worth replicating must naturally be weighed against
    the need for unbiased sampling from the population of original
    studies. Thus, whether a study - in isolation - is worth replicating
    is not always the guiding principle for study selection.}
  \end{itemize}
\item
  Arbitrariness (i.e.~picking a random study)

  \begin{itemize}
  \tightlist
  \item
    \emph{Reply: This goal makes sense in scenarios like the one
    described in the previous point. When the goal is to estimate
    average replicability in a field, random/arbitrary selection is
    necessary to ensure valid inferences about replicability to the
    population of psychology research. It is an interesting factor since
    it essentially prevents you from selecting on any other criteria.
    For a broader discussion of the benefits of random study selection,
    see Kuehberger, \& Schulte-Mecklenbeck, (2018). }
  \end{itemize}
\item
  Teaching/pedagogy

  \begin{itemize}
  \tightlist
  \item
    \emph{Reply: Several people brought this up during discussion. It
    nicely illustrates a point that one discussion group put a lot of
    emphasis on: the goal of the replication study will partially
    determine what is worth replicating. I.e. you cannot adequately
    answer what should be replicated without first defining what you
    hope the replication will achieve. For example, if our goal is to
    reduce uncertainty, then we likely want to replicate original
    studies that are highly uncertain. However, if our goal is
    pedagogical, we might want to replicate studies with highly certain
    outcomes (e.g.~to demonstrate a principle, or to know when a study
    design was successfully implemented), even if replicating them does
    not reduce our uncertainty much.}
  \end{itemize}
\item
  Some findings are reliable enough to be used as manipulations in later
  experiments

  \begin{itemize}
  \tightlist
  \item
    \emph{Reply: This reminds me of replications that are done because
    the authors are motivated, not to replicate a finding, but to change
    or extend a study procedure (this was also mentioned by several
    people in our discussion). It is important to note that in cases
    where replication is just a minor step in reaching a larger goal, we
    may not be replicating to reduce our uncertainty about the results
    of prior research. In fact, we may actually prefer the results we
    are replicating to already be highly replicable (e.g.~when we reuse
    a previously validated measurement tool). }
  \end{itemize}
\end{itemize}

\hypertarget{other-interesting-comments-suggestions-and-questions-and-our-replies.}{%
\subsection{Other interesting comments, suggestions, and questions (and
our
replies).}\label{other-interesting-comments-suggestions-and-questions-and-our-replies.}}

\begin{itemize}
\item
  When do we say ``there is no need of further replication'' of a study
  or theoretical construct?

  \begin{itemize}
  \item
    \emph{Peder: This is a good and difficult question, and it relates
    to a few other comments made in the session. In general, you
    probably always need to consider this question as relative to other
    replication studies you could do. I.e. once you have conducted a
    replication of study A, is it worth replicating A again, or is there
    now a study B that has become more important to replicate than A,
    given that A is corroborated/falsified by the replication you just
    conducted? }
  \item
    \emph{Anna: this also reminds me that we as researchers are
    generally bad at knowing how to weigh the evidence we are looking
    at, perhaps because we are not used to calibrating our conclusions
    with the evidence, or because we are still catching up when it comes
    to reporting and expressing our certainty in terms of confidence
    intervals. For a good read on planning your sample size on the basis
    of how accurate you want you estimate to be,
    \href{10.1146/annurev.psych.59.103006.093735}{see Maxwell, Kelley \&
    Rausch, 2008}.}
  \end{itemize}
\item
  Most replications are probably conducted by students interested in the
  subject matter at hand (which might incidentally include some of these
  factors like influence, importance, likelihood etc.).

  \begin{itemize}
  \tightlist
  \item
    \emph{Reply: An important point. An implicit assumption behind our
    work is that researchers often have multiple replication candidates
    that would be equally interesting to replicate. Alternatively, you
    could perhaps assume that personal interest is not as important as
    replicating research with societal impact, highly uncertain results,
    etc., so personal interest should not factor into the equation. But
    these assumptions could of course be false, and it is certainly the
    case that many researchers select studies based on personal interest
    (see
    e.g.~\url{https://pedermisager.netlify.app/post/what-to-replicate/}).}
  \end{itemize}
\item
  Retrospective registered report: If you get a paper results blind,
  would the method and rationale convince you that you want to run a
  replication?

  \begin{itemize}
  \tightlist
  \item
    \emph{Reply: A very intriguing idea! To offer one caveat, I do think
    that results are sometimes important for knowing if a study is worth
    replicating. For example, if it turns out that the results of a
    study are extremely accurate because of very low variance in the
    estimates then one might want to conclude that this study does not
    need replication. However, one would need the results to know
    whether this is the case. On the other hand, one could perhaps
    assume that variance is mainly determined by factors such as sample
    size, which are known before seeing the results. }
  \end{itemize}
\item
  Should we really buy into the idea of `not everything can be
  replicated'? If a research question was worth asking, worth funding,
  worth using participants/animals/resources to answer--isn't it worth
  building in replication from the start? That is, instead of selecting
  completed projects to replicate, shouldn't we design projects with
  internal and/or cross-lab replications from the start?

  \begin{itemize}
  \tightlist
  \item
    \emph{Reply: Of course we should! The question is, whether we still
    want existing literature to factor into what we study. If we do, we
    need to know its robustness before we implement build-in
    replications/validations.}
  \end{itemize}
\end{itemize}

\hypertarget{links-to-materials.}{%
\subsection{Links to materials.}\label{links-to-materials.}}

PDF versions of all documents and notes pertaining to our session can be
found on this OSF project page: \url{https://osf.io/ufea4/}. If you'd
like a more detailed introduction to our, the session hosts', beliefs
about what is worth replicating, I recommend checking out the youtube
recording in a previous post:
\url{https://pedermisager.netlify.app/post/choosing-what-to-replicate-talk/}.

We want to thank everyone involved in our unconference session for their
time and contributions!

\end{document}
